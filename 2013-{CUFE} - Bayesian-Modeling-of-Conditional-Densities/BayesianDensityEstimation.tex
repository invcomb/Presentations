\documentclass[10pt]{beamer}

\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

%% Fonts
\usepackage{multicol}
\usepackage{mathabx}
\usepackage[scaled]{helvet}
\usepackage{lmodern}
\usepackage{eulervm}
\usefonttheme[onlymath]{serif}
\usefonttheme{professionalfonts}
\usefonttheme{structurebold}

\usepackage{hyperref}

\usepackage{bm}
\usepackage{pgf}
\usepackage{natbib}

%% Color & Theme
\definecolor{SUblue}{RGB}{0,0,180}
\usecolortheme[RGB={0,0,180}]{structure}
\usetheme{Boadilla}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[circle]
\setbeamerfont{title}{size=\large}
\setbeamerfont{frametitle}{size=\large}
\setbeamerfont{framesubtitle}{size=\large,shape =$\color{violet}{\looparrowdownright}~$}
\setbeamercolor{title}{fg=white, bg= SUblue!75!green}
\setbeamercolor{framesubtitle}{fg=violet}
\setlength{\leftmargini}{5pt}


%\hypersetup{colorlinks=true, linkcolor=blue}

\title[Conditional Densities]{Bayesian Modeling of Conditional Densities}
%\subtitle{--- the statistical approach}

\author[Feng Li]{\textbf{Feng Li}\\\texttt{ <feng.li@cufe.edu.cn>}}
\institute[StatMath, CUFE]{\footnotesize{\textbf{School of Statistics and
      Mathematics\\ Central University of Finance and Economics}}}
\date{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


%% Title page
\begin{frame}[plain]
    \titlepage
  \tiny{Revision: \today}
\end{frame}



%% Outline page
\section*{Outline}
\begin{frame}
  \frametitle{Outline}
  \tableofcontents
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Conditional density models}
\begin{frame}
  %\frametitle{Knowing the elephant}
  \frametitle{The trend of statistical modeling}

  \begin{itemize}
  \item In the 1950s, linear regression model  was considered as very advanced
    which is now the standard course content for university students.
  \item The data are much more complicated nowadays we meet.
    \begin{itemize}
    \item Numerical, categorical, brain image...
    \item A few observations to millions by millions.
    \item Very high-dimensional data are not rare anymore.
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Density estimation}
  \begin{itemize}
  \item \textbf{Density estimation} is the procedure of estimating an unknown density
    $p(y)$ from observed data

  \item Histogram, kernel methods, splines, wavelets are all density estimation
    methods.

  \item \textbf{Mixture models} \citep{jiang1999approximation} have become a popular alternative approach,
    \[
    p(y|\theta)=\sum\limits _{k=1}^{K}\omega_{k}p_{k}(y|\theta_{k}),
    \]
    where $\sum\nolimits _{k=1}^{K}\omega_{k}=1$ for non-negative mixture
    \textbf{weights} $\omega_{k}$ and $p_{k}(x|\theta_{k})$ are the \textbf{
      component densities}.

  \item If $K=\infty$, it is called an \textbf{infinite mixture} \citep{escobar1994estimating}, the
    \textbf{Dirichlet process mixture} being the most prominent example.

  \item Mixture densities can be used to capture data characteristics such as
    multi-modality, fat tails.

  \end{itemize}

\end{frame}


\begin{frame}
  \begin{figure}
\centering
\includegraphics[width=10cm]{MixtureOfNormals2}\caption{Using mixture of normal
  densities (thin lines) to mimic a flexible density (bold line).}
\label{fig: mix-norm}
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Conditional density estimation}

  \begin{itemize}
  \item The \textbf{conditional density estimation} concentrates on modeling the
    relationship between a response $y$ and set of covariates $x$ through a
    conditional density function $p(y|x)$

  \item Mixtures of conditional densities is the obvious extension of mixture
    models to the conditional density estimation problem:
    \[
    p(y|x)=\sum\nolimits _{k=1}^{K}\omega_{k}p_{k}(y|x)
    \]
    where $p_{i}(y|x)$ is the conditional density in $i$:th mixture component.

  \item A \textbf{smooth mixture} is a finite mixture density with weights that
    are smooth functions of the covariates
    \[
    \omega_{k}(x)=\frac{\exp(x'\gamma_{k})}{\sum_{i=1}^{K}\exp(x'\gamma_{i})}.
    \]
  \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Conditional density estimation}

  \begin{itemize}
  \item In conditional density estimation, an important focus is modeling
    the regression mean $E(y|x)$.

  \item A \textbf{spline} is a popular approach for nonlinear regression that
    models the mean as a linear combination of a set of nonlinear basis
    functions of the original regressors \citep{holmes2003generalized},
    \[
    y=f(x)+\epsilon=x'\beta+\sum\nolimits _{i=1}^{k}x(\xi_{i})'\beta_{i}+\epsilon
    \]

  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Multivariate density estimation with copulas}
  \begin{itemize}
  \item The \textbf{multivariate density estimation} and conditional density estimation
    are analogues of their univariate cases except that the densities
    $p(\bm{Y})$ and $p(\bm{Y}|\bm{X})$ are multivariate.

  \item In addition to the methods mentioned above, a \textbf{copula function}
    separates the multivariate dependence from its marginal functions, and it
    is possible to use both continuous and discrete marginal models.

  \item Let $F(y_{1},...,y_{M})$ be a multi-dimensional distribution function
    with marginal distribution functions
    $F_{1}(y_{1}),\cdots,F_{M}(y_{M})$. Then there exists a copula function $C$
    \citep{sklar1959fonctions} such
    that
\[
\begin{split}F(y_{1},...,y_{M})= & C(F_{1}(y_{1}),...,F_{M}(y_{M}))\\
= & C\left(\int_{-\infty}^{y_{1}}f_{1}(z_{1})dz_{1},...,\int_{-\infty}^{y_{M}}f_{M}(z_{M})dz_{M}\right)=C(u_{1},...,u_{M})
\end{split}
\]
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Multivariate density estimation with copulas}

  \begin{itemize}
  \item The \textbf{Kendall's $\tau$ correlation} between two marginal
    densities can be measured by Kendall's $\tau$
    \[
    \tau=4\int\int F(y_{1},y_{2})\mathrm{d}F(y_{1},y_{2})-1=4\int\int
    C(u_{1},u_{2})\mathrm{d}C(u_{1},u_{2})-1.
    \]

  \item \textbf{Tail-dependence} measures the extent to which
several variables simultaneously take on extreme values
\[
\begin{split}\lambda_{L}= & \lim\limits _{u\to0^{+}}Pr(X_{1}<F_{1}^{-1}(u)|X_{2}<F_{2}^{-1}(u))=\lim\limits _{u\to0^{+}}\frac{C(u,u)}{u},\\
\lambda_{U}= & \lim\limits _{u\to1^{-}}Pr(X_{1}>F_{1}^{-1}(u)|X_{2}>F_{2}^{-1}(u))=\lim\limits _{u\to1^{-}}\frac{1-C(u,u)}{1-u}.
\end{split}
\]

\item Modeling tail-dependence is an very important topic in econometrics
  \citep{joe1997multivariate} \citep{patton2012review}.

  \end{itemize}



\end{frame}





\begin{frame}
  \frametitle{The Bayesian approach for modeling density features}
  \framesubtitle{A feature of a density}

  \begin{itemize}
  \item We use the word \textbf{feature} to describe a characteristic of a
    density.

  \item In GLM or splines, $\mu = \eta (X\beta)$ is the feature that describes the \textbf{mean}.

  \item In mixtures contents, the \textbf{mean}, \textbf{variance},
    \textbf{skewness} and \textbf{kurtosis} are features of each component
    density.

  \item In copula modeling, the \textbf{tail-dependence} and
    \textbf{correlation} are two features of interest.

  \item We allow each of the features are connected to covariates as
    \begin{equation*}
  \begin{split}
    \mu & =  \beta_{\mu0}+x_{t}'\beta_{\mu} \\
    \ln\phi & =  \beta_{\phi0}+x_{t}'\beta_{\phi} \\
    \ln\lambda & =  \beta_{\lambda0}+x_{t}'\beta_{\lambda} \\
    \ln\nu & = \beta_{\nu0}+x_{t}'\beta_{\nu}\\
    \lambda_L & = \varphi_{\lambda}^{-1}(X\beta_\lambda)\\
 \tau &= \varphi_{\tau}^{-1}(X\beta_\tau).
  \end{split}
\end{equation*}

\item This approach allows the feature to be dynamic and interpretable
  friendly.

\item We only need to sample the posterior of $p(\beta|Data)$.

  \end{itemize}

\end{frame}

\section{Bayesian approach for modeling conditional density}


\begin{frame}
  \frametitle{The Bayesian approach for modeling density features}
  \framesubtitle{The general MCMC scheme}

  \begin{itemize}

  \item The model settings are very complicated now.

  \item Sampling the posterior requires an efficient MCMC method.

  \item We update all the parameters jointly by using
    Metropolis-Hastings within Gibbs.
  \item The proposal density for each parameter vector $\beta$ is a multivariate \emph{t}-density with  $df>2$,
    \[
    \bm{\beta}_{p} |\bm{\beta}_{c}\sim\bm{MVT}\left[\bm{\hat{\beta}},~\left.-\left(\frac{\partial^{2}\ln
            p(\bm{\beta}|\bm{Y})}{\partial\bm{\beta}\partial\bm{\beta}^{\prime}}\right)^{-1}\right\vert
      _{\bm{\beta}=\bm{\hat{\beta}}},~df\right],
    \]
    where $\bm{\hat{\beta}}$ is obtained by $R$ steps ($R\leq 3$) Newton's
    iterations during the proposal with analytical gradients.

  \item Variable selections are carried out simultaneously.

  \item \textbf{The key:} The analytical gradients require the derivative for
    the copula density and marginal densities.

  % \item It is eventually straightforward. Thanks to the chain rule!

  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Regularization via Bayesian variable selection}

  \begin{itemize}
  \item \textbf{Variable selection} is commonly to select meaningful covariates that
    contributes to the model, inhibit ill-behaved design matrices, and to
    prevent model over-fitting.

  \item A standard Bayesian variable selection approach \citep{nott2005adaptive} is to augment the
    regression model with a variable selection indicator $\mathcal{I}$ for each
    covariate
\[
\mathcal{I}_{j}=\begin{cases}
1 & \text{if \ensuremath{\beta_{j}\neq0}}\\
0 & \text{if \ensuremath{\beta_{j}=0}},
\end{cases}
\]
where $\beta_{j}$ is the $j$th covariate in the model.


\item Variable selection is then obtained by sampling the posterior
  distribution of all regression coefficient jointly with the variable
  selection indicators, thereby yielding the marginal posterior probability of
  variable inclusion $p(\mathcal{I}|\text{Data})$.

  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Regularization via shrinkage estimator}

  \begin{itemize}
  \item A \textbf{shrinkage estimator} shrinks the regression coefficients towards zero
    rather than eliminating the covariate completely.
  \item LASSO can be viewed as regression with a Laplace prior.

  \item One way to select a proper value of the shrinkage is by
    cross-validation, which is costly with big data and complicated models.

  \item In the Bayesian approach, the shrinkage parameter is usually
    automatically estimated together with other parameters in the posterior
    inference.

  \item Shrinkage and variable selection can be used \textbf{simultaneously}.

  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Bayesian predictive inference}
  \begin{itemize}
  \item Assuming that the data observations are independent conditional on the
    model parameters $\theta$, the \textbf{predictive density} can be written
\[
p(Y_{b}|Y_{-b})=\int\prod_{j=1}^{n}p(Y_{j,b}|\theta)p(\theta|Y_{-b})\mathrm{d}\theta
\]

\item For a time series the forecast can instead be based on the
  decomposition
  \[
  \begin{split}
    p(y_{T+1},..,y_{T+T^{\ast}}|y_{1},..,y_{T})=&p(y_{T+1}|y_{1},..,y_{T})\times
    \cdots\\
    &\times p(y_{T+T^{\ast}}|y_{1},..,y_{T+T^{\ast}-1}),
  \end{split}
  \]
  with each term in the decomposition
  \[ p(y_{t}|y_{1},..,y_{t-1})=\int
  p(y_{t}|y_{1},..,y_{t-1},\theta)p(\theta|y_{1},..,y_{t-1})\mathrm{d}\theta,\]


  \item \textbf{The prediction error} at $x_0$ can be decomposed as three parts
    \begin{equation*}
      \begin{split}
        EPE(x_0) &= E((Y-\hat f(x_0))^2|X=x_0)\\
        &=\sigma^2 + Bias^2(\hat f(x_0)) + Var(\hat f(x_0))
      \end{split}
    \end{equation*}
    which is the so-called the \textbf{bias-variance trade-off}.

\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Bayesian model comparison}

  \begin{itemize}
  \item Bayesian model comparison have historically been based on the marginal
    likelihood \citep{kass1995bayes}.

  \item However,that the marginal likelihood is very sensitive to the
    specification of prior.

  \item A more prominent tool for model comparisons is based on the \textbf{log
    predictive density score} (LPDS)
    \[
    \mathrm{LPDS}=\frac{1}{B}\sum\nolimits _{i=1}^{B}\log p(Y_{b_{i}}|Y_{-b_{i}})
    \]

  \item In Bayesian framework, as the whole posterior of parameters can be
    obtained, model consistency evaluation does not rely one large sample
    properties.

  \item There are still consistency studies on issues like variable
    selections \citep{casella2009consistency}.

  \end{itemize}

\end{frame}


\section{Modeling nonlinear mean with splines}

%\section{Modeling nonlinear mean with splines to firm leverage data}
\begin{frame}
  \frametitle{Modeling nonlinear mean with splines to firm leverage data}
  \framesubtitle{The data}
  \vspace{0.25cm}
  \scalebox{0.65}{
    \begin{tabular}{rl}
      \textbf{leverage ($Y$):} &total debt/(total debt+book value of equity), 4405 observations;\\
      \textbf{tang:} & tangible assets/book value of total assets;\\
      \textbf{market2book:} &(book value of total assets - book value of equity +
      market value of equity) / book value of total assets;\\
      \textbf{logSales:} &logarithm of sales;\\
      \textbf{profit:} &(earnings before interest, taxes, depreciation, and amortization) /
      book value of total assets.    \\
    \end{tabular}}
  \vspace{0.25cm}
  \begin{center}
    \begin{figure}
      \includegraphics[width=\textwidth]{Rajanscatter.eps}
    \end{figure}
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{The multivariate surface model}
  \framesubtitle{The model}
  \begin{itemize}
  \item Splines are regression models with flexible \textbf{mean functions} by
    selecting and placing knots to covariates space.

  \item The multivariate surface spline model \citep{li2013efficient} consists of three different components,
    {\color{blue}\emph{linear}}, {\color{blue}\emph{surface}} and
    {\color{blue}\emph{additive}} as
    \[\begin{gathered}
      \bm{Y}=\bm{X}_o\bm{B}_o+
      \bm{X}_s(\xi_s)\bm{B}_s+\bm{X}_a(\xi_a)\bm{B}_a + \bm{E}.
    \end{gathered}\]
  \item We treat the knots $\xi_i$ as unknown parameters and let them move
    freely.
    \begin{itemize}
    \item A model with a minimal number of free knots outperforms model with lots
      of fixed knots.
    \end{itemize}
  \item For notational convenience, we sometimes write model in compact form
    \[
    \bm{Y}=\bm{X}\bm{B}+\bm{E},
    \]
    where $\bm{X}=[\bm{X}_{o},\bm{X}_{s},\bm{X}_{a}]$ and
    $\bm{B}=[{\bm{B}_{o}}',{\bm{B}_{s}}',{\bm{B}_{a}}']'$ and $\bm{E}\sim \bm{N}_p(\bm{0},~\bm{\Sigma})$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The multivariate surface model}
  \framesubtitle{The prior}
  \begin{itemize}
  \item Conditional on the knots, the prior for $\bm{B}$ and
    $\bm{\Sigma}$ are set as
    \[
    \begin{split}
      \mathrm{vec}\bm{B}_i |\bm{\Sigma},~\bm{\lambda}_i &\sim \bm{N}_q\left[
        \mu _i, ~\bm{\Lambda}_i^{1/2} \bm{\Sigma} \bm{\Lambda}_i^{1/2} \otimes
        \bm{P}_i^{-1}
      \right], ~i\in \{o,s,a \},\\
      \bm{\Sigma} &\sim \bm{IW} \left[n_0 \bm{S}_0,~n_0\right],
    \end{split}
    \]
    \begin{itemize}

    \item $\bm{\Lambda}_i=diag(\bm{\lambda}_i)$ are called the shrinkage parameters,
      which is used for overcome overfitting through the prior.
    \item If $\bm{P}_i=\bm{I}$, can
      prevent singularity problem, like the ridge regression estimate.

    \item If $\bm{P}_i=\bm{X}_i'\bm{X}_i$: use the covariates
      information, also a compressed version of least squares estimate when
      $\bm{\lambda}_i$ is large.
    \end{itemize}
  \item The shrinkage parameters are estimated in MCMC
    \begin{itemize}
    \item A small $\bm{\lambda}_{i}$ shrinks the variance of the conditional posterior
      for $\bm{B}_i$
    \item It is another approach to selection important variables (knots) and
      components.
    \end{itemize}
  \item We allow to mixed use the two types priors ( $\bm{P}_i=\bm{I}$,
    $\bm{P}_i=\bm{X}_i'\bm{X}_i$) in different components in order to take the both
    the advantages of them.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The multivariate surface model}
  \framesubtitle{The Bayesian posterior}
  \begin{itemize}
  \item The posterior distribution is conveniently decomposed as
    \[
    p(\bm{B},\bm{\Sigma},\bm{\xi},\bm{\lambda}|\bm{Y},\bm{X})=p(\bm{B}|\bm{\Sigma},\bm{\xi},\bm{\lambda},\bm{Y},\bm{X})p(\bm{\Sigma}|\bm{\xi},\bm{\lambda},\bm{Y},\bm{X})p(\bm{\xi},\bm{\lambda}|\bm{Y},\bm{X}).
    \]
  \item Hence $p(\bm{B}|\bm{\Sigma},\bm{\xi},\bm{\lambda},\bm{Y},\bm{X})$ follows
    the multivariate normal distribution according to the conjugacy;
  \item When $p=1$, $p(\bm{\Sigma}|\bm{\xi},\bm{\lambda},\bm{Y},\bm{X})$ follows the inverse Wishart distribution
    \[
    \bm{IW} \left[n_0+n, \left\{{n_0}{\bm{S}_0} + n \tilde {\bm{S}} +
        \sum\limits_{i \in \{ o,s,a\} } {{\bm{\Lambda}}_i^{-1/2}({{\tilde
              {\bm{B}}_i}} - {\bm{M}_i})'{\bm{P}_i}({{\tilde {\bm{B}}}_i} -
          {\bm{M}_i}){\bm{\Lambda}}_i^{ - 1/2}}\right\} \right]
    \]
  \item When $p \geq 2$, no closed form of
    $p(\bm{\Sigma}|\bm{\xi},\bm{\lambda},\bm{Y},\bm{X})$, the above result is a
    very accurate approximation. Then the marginal posterior of $\bm{\Sigma}$,
    $\bm{\xi}$ and $\bm{\lambda}$ is
    \[\begin{split}
      p\left(\bm{\Sigma}, {\bm{\xi} ,\bm{\lambda} |\bm{Y},\bm{X}} \right) =
      &c\times p(\bm{\xi},\bm{\lambda})\times {|
        {{\bm{\Sigma}_{\bm{\beta}} }} |^{ - 1/2}}{| \bm{\Sigma} |^{ - (n + {n_0} + p +
          1)/2}}{| {{\bm{\Sigma}_{\bm{\tilde
                \beta} }}} |^{ - 1/2}}\\
      &\times \exp \left\{ { - \frac{1}{2}\left[ {\mathrm{tr}{\bm{\Sigma}^{ - 1}}\left(
                {{n_0}{\bm{S}_0} + n\bm{\tilde S}} \right) + \left( {\bm{\tilde \beta} -
                  \bm{\mu} } \right)'{\bm{\Sigma}_{\bm{\beta}}^{-1} }\left( {\bm{\tilde \beta} - \bm{\mu} }
              \right)} \right]} \right\}
    \end{split}\]
  \end{itemize}
\end{frame}



\begin{frame}[plain]
  \begin{multicols}{2}
    \begin{center}
      \begin{figure}
        \includegraphics[width=0.5\textwidth]{RajanLPDS_SurfaceBesideAdditive}
      \end{figure}
    \end{center}
    \begin{enumerate}
    \item [$\uparrow$] \tiny{Models with only surface or additive components}
    \item [$\rightarrow$] \tiny{Model with both additive and surface
        components.}
    \item [LPDS] \tiny {Log predictive density score which is defined as
        \[
        \begin{split}
          LPDS=&\frac{1}{D}\sum\nolimits_{d=1}^{D}\ln
          p(\tilde{\bm{Y}}_{d}|\tilde{\bm{Y}}_{-d},\bm{X})\\
          =&\int\!\prod\nolimits_{i\in\tau_{d}}p(\bm{y}_{i}|\bm{\theta},
          \bm{x}_{i})p(\bm{\theta}|\tilde{\bm{Y}}_{-d})\mathrm{d}\bm{\theta},
        \end{split}
        \]
        and $D=5$ in the cross-validation.}
    \end{enumerate}

    \begin{center}
      \begin{figure}
        \includegraphics[height=\textheight]{RajanLPDS_SurfacePlusAdditive}
      \end{figure}
    \end{center}
  \end{multicols}
\end{frame}

% \begin{center}
%   \begin{figure}
%     \includegraphics[width=0.5\textwidth]{RajanLPDS_SurfaceBesideAdditive} \includegraphics[height=\textheight]{RajanLPDS_SurfacePlusAdditive}
%   \end{figure}
% \end{center}
% \end{frame}

\begin{frame}[plain]
  %% \frametitle{Application to firm leverage data}
  \framesubtitle{The posterior locations for knots}
  \begin{center}
    \begin{figure}
      \includegraphics[height=\textheight]{RajanPostKnots.eps}
    \end{figure}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Modeling nonlinear mean with splines to firm leverage data}
  \framesubtitle{Posterior mean surface(left) and standard deviation(right)}
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.5\textwidth]{RajanPostMean.eps}~~~\includegraphics[width=0.5\textwidth]{RajanPostSD.eps}
    \end{figure}
  \end{center}
\end{frame}

\section{Can we have a model that is big like an elephant?}

\begin{frame}
%  \frametitle{Knowing the elephant}
  \frametitle{Can we have a model that is big like an elephant?}

      \begin{figure}
        \centering
        \includegraphics[height=0.8\textheight]{elephant}
      \end{figure}
\tiny{by John Godfrey Saxe (1816-1887)}
\end{frame}


\begin{frame}
  \frametitle{Knowing the elephant}

  \begin{itemize}
  \item Sophisticated models are essential for such situations.
  \item In principle, the complicated model should be able to capture more
    complicated data features.
  \item Estimating such model is not easy.
  \item There is huge space to explore.
    \begin{itemize}
    \item The computer is still not fast enough.
    \item Techniques like parallel
      computing should be used to speed up the computation.
    \item Statistics with big data is the new challenge.
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{References}
  \begin{small}
    %\phantomsection
    %\addcontentsline{toc}{section}{\bibname}
    \def\newblock{\hskip .11em plus .33em minus .07em} % important line
    \bibliographystyle{biometrika}
    \bibliography{full,References}
  \end{small}
\end{frame}


\begin{frame}[plain]

  \addtocounter{framenumber}{-1}

\emph{...essentially, all models are wrong, but some are useful}\\
\hfill --- George E. P. Box
\vspace{1cm}
  \begin{center}
    {\color{SUblue} \textbf{\Huge Thank you!}}
  \end{center}

\end{frame}



\end{document}

\end{document}
