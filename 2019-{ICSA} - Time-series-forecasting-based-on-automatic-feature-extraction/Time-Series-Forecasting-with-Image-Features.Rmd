---
title: "Feature-based time series generation and forecasting"
subtitle: "Feng Li"
author: "with Yanfei Kang and Rob Hyndman"
date: "June 13, 2018"
output:
  beamer_presentation:
    fig_caption: yes
    includes:
      in_header: header.tex
    incremental: no
    keep_tex: yes
    slide_level: 2
    theme: metropolis
  slidy_presentation:
    incremental: no
bibliography: MARpaper.bib
fontsize: 12pt
classoption: compress
---


# Motivation

## 

\centerline{\includegraphics[width=\textwidth]{figures/motivation}}

## The No-Free-Lunch theorem 

- There is never universally best method that fits in all situations [@wolpert1996lack].

- The explosion of new algorithms development makes the question even more worth focusing.

- No single forecasting method stands out the best for any type of time series [@adam1973individual].

# Literature

## Literature 

- Features of time series $\rightarrow$ benefits in producing more accurate forecasting accuracies [@adam1973individual] 

- Features $\rightarrow$  forecasting method selection rules [@meade2000evidence]

- "Horses for courses" $\rightarrow$ effects of time series features to the forecasting performances [@Petropoulos2014]

-  Our published work: visualize the performances of different forecasting methods in a 2D space $\rightarrow$ better understanding of their relative performances [@kang2017visualising]


## Existing problems 

- Few studies have concluded a selection process that beats **single-method** forecasting strategies.
    - select a good, but not necessarily the best method [@meade2000evidence]
    - impossible to identify one single optimal method based on features [@Petropoulos2014]
    - space can not be separated nicely according to the strengths and weaknesses of forecasting methods [@kang2017visualising]

## Possible reasons?

- inadequate features

- limited training time series data (not only in number, but in diversity)




# What we do?

## The framework

\centerline{\includegraphics[width=\textwidth]{figures/framework.pdf}}

## Final aim: forecasting on testing data

#### Testing data: M3 data [@makridakis2000m3]

- 3003 time series
- From demography, finance, business and economics
- Lengths between 14 and 126
- Either non-seasonal, monthly or quarterly
- Positive

## Questions to be answered

- What time series features?
- How to construct time series training data?
- 2D projection?
- How to model features and forecasting methods?
- How to generate new time series with certain features?




# Time series features

## Basic idea

Transform a given time series $\{x_1, x_2, \cdots, x_n\}$ to a feature vector $F = (F_1, F_2, \cdots, F_p)'$ [@kang2017visualising; @cikm2015]. 

#### A feature $F_k$ can be any kind of function computed from a time series:

1. A simple mean
2. The parameter of a fitted model
3. Some statistic intended to highlight an attribute of the data
4. ...

## Which features should  we use?

- There does not exist the best feature representation of a time series [@fulcher2018feature].
- Depends on both the **nature** of the time series being analysed, and the **purpose** of the analysis. 
\pause

    - With unit roots, the mean is not a meaningful feature without some constraints on the initial values. \pause
    
    - CPU usage every minute for a large number of servers: we observe a daily seasonality. The mean may provide useful comparative information despite the time series not being stationary.
 
## Which features should  we use?

- Time series are of different lengths, on different scales, and with different properties.
- We restrict our features to be ergodic, stationary and independent of scale. 
- 17 sets of diverse features.
- New features are intended to measure attributes associated with multiple seasonality, non-stationarity and heterogeneity of the time series.


## Multiple seasonal time series
```{r cashdata, fig.height=3, fig.cap='Daily cash money demand at some automatic teller machine.', message=FALSE, echo=FALSE, warning=FALSE}
library(tscompdata)
library(ggplot2)
library(tsfeatures)
options(dplyr.width = Inf)
nn5$`NN5-001`%>%autoplot(ylab = 'Cash')
```

## Features for multiple seasonal time series 

### STL decompostion extension
$$ x_t = f_t + s_{1,t} + s_{2,t} + \cdots + s_{M,t} + e_t.$$
The strength of trend can be measured by:
$$
    F_{10} = 1- \frac{\text{var}(e_t)}{\text{var}(f_t + e_t)}.
$$

The strength of seasonality for the $i$th seasonal component:

$$
F_{11,i} = 1- \frac{\text{var}(e_t)}{\text{var}(s_{i,t} + e_t)}.
$$



## Features on heterogenity

1. Pre-whiten the time series $x_t$ to remove the mean, trend, and Autoregressive (AR) information.
3. Fit an GARCH(1,1) model on the pre-whitened time series $y_t$ to measure for the ARCH effects.
4. Test for the arch effects in the obtained residuals $z_t$ using a second GARCH(1,1) model. 

## Features on heterogenity
\metroset{block=fill} 
\begin{alertblock}{The features of time series on heterogenity.}
\begin{enumerate}
\item The sum of squares of the first 12 autocorrelations of $\{y_t^2\}$.
\item The sum of squares of the first 12 autocorrelations of $\{z_t^2\}$.
\item The $R^2$ value of an AR model applied to $\{y_t^2\}$.
\item The $R^2$ value of an AR model applied to $\{z_t^2\}$.
\end{enumerate} 
\end{alertblock}



## Time series features we use

\centerline{\includegraphics[width=1.1\textwidth]{figures/tsfeatures.pdf}}

## An example 
\fontsize{7}{8}\sf
```{r cashfeatures, fig.height=3, fig.cap='Daily cash money demand at some automatic teller machine.', message=FALSE, echo=FALSE, warning=FALSE}
options(dplyr.width = Inf)
nn5$`NN5-001`%>%autoplot(ylab = 'Cash')
features <- c(
  "entropy",
  "acf_features",
  "pacf_features",
  "stl_features",
  "ndiffs",
  "nsdiffs",
  "hurst",
  "nonlinearity",
  "heterogeneity"
)
print(as.data.frame(tsfeatures::tsfeatures(nn5$`NN5-001`, features = features, na.action = na.interp)))
```

# Training data simulation


## Gaussian Mixure Autoregressive (MAR) models

- Consist of multiple stationary or non-stationary autoregressive components.  
- A $K$-component MAR model is defined as [@wong2000mixture]:
$$
F(x_t|\mathcal{F}_{t-1}) =
\sum\limits_{k=1}^K\alpha_k\Phi(\frac{x_t-\phi_{k0}-\phi_{k1}x_{t-1}-\cdots
-\phi_{kp_k}x_{t-p_k}}{\sigma_k}),
$$
where $F(x_t|\mathcal{F}_{t-1})$ is the conditional cumulative
distribution of $x_t$ give the past information $\mathcal{F}_{t-1}$.
$\Phi(\cdot)$ is the cumulative distribution function of the standard normal
distribution. $\sum_{k=1}^K \alpha_k= 1$, where $\alpha_k > 0$, $k = 1, 2,
\cdots, K$. 

## Conditional mean and variance

$$E(x_t|\mathcal{F}_{t-1}) =
\sum\limits_{k=1}^K\alpha_k(\phi_{k0}+\phi_{k1}x_{t-1}+\cdots+\phi_{kp_k}x_{t-p
_k}) = \sum\limits_{k=1}^K\alpha_k \mu_{k, t}.$$

$$\mathrm{var}(x_t|\mathcal{F}_{t-1}) = \sum\limits_{k=1}^K\alpha_k \sigma_k^2 +
\sum\limits_{k=1}^K\alpha_k \mu_{k, t}^2 - \left(\sum\limits_{k=1}^K\alpha_k
\mu_{k, t}\right)^2.
$$

- $\mathrm{var}(x_t|\mathcal{F}_{t-1})$ changes with conditional means of different components.
- The shape of the conditional distributions of the time series changes with time.
- The MAR models can handle heteroscedasticity, which is common in financial time series.

## Other merits of MAR models

- Mixtures of stationary and non-stationary components can yield a stationary process.
- To handle non-stationary time series, one can just include a unit root in each component.
- Possible to capture more (or any) time series features, since different specifications of finite mixtures have been shown to be able to approximate large nonparametric classes of conditional multivariate densities [@jiang1999on].

## Simulation settings

\centerline{\includegraphics[width=\textwidth]{figures/simSettings.pdf}}

## Visualisation in 2D space

#### t-Stochastic Neighbor Embedding (t-SNE)
- Main idea: convert the distances to conditional probabilities and minimize the mismatch (kullback-Leibler divergence) between probabilities before and after the mapping.
- Nonlinear, and retaining both local and global structure [@maaten2008visualizing]

#### PCA
- Linear, and putting more emphasize on keeping dissimilar data points far apart


## Investigating the coverage of MAR models

\centerline{\includegraphics[width=0.9\textwidth]{figures/coverage.png}}


## Miscoverage 

We define the miscoverage of dataset A over dataset B as:

1. Find the maximum ranges of the $x$ and $y$ axes reached by the two datasets A and B, and cut the $x$ and $y$ dimensions into $N_b = 30$ bins.
2. In the constructed two-dimensional grid with $N_b^2 = 900$ subgrids, we denote $\mathcal{I}_{i,A} = 0$ if no points in dataset A fall into the $i$th subgrid. $\mathcal{I}_{i,A} = 1$ otherwise. The same defination of $\mathcal{I}_{i,B}$ applies for dataset B.
3. The miscoverage of dataset A over dataset B is defined as $$\text{miscoverage}_{A/B} = \frac{\sum\limits_{i = 1}^{N_b}[(1 - \mathcal{I}_{i,A})*\mathcal{I}_{i,B}]}{N_b^2}.$$

## Miscoverage

\centerline{\includegraphics[width=\textwidth]{figures/miscoverage.png}}


<!-- ## t-SNE *v.s.* PCA -->

<!-- \centerline{\includegraphics[width=0.8\textwidth]{figures/ComparePCAtsne.png}} -->

# Extension to multiple seasonal time series


## Simulation of multiple seasonal time series

\centerline{\includegraphics[width=0.6\textwidth]{figures/daily.png}}



# New time series generation based on MAR models


## New time series generation 

- Time series $\rightarrow$ features \checkmark

- Time series $\leftarrow$ features ?
    - Genetic Algorithm (GA) to evolve time series with length n 
    - GA to tune the MAR model parameters $\Theta = (\alpha_k, \phi_i)$


## GA procedure


- Firstly decide on the period $P$ and length $n$.
- Given a target $T_i$ in the feature space. Find $\Theta^{*}$ that can simulate $X_{T_i}$
with its feature vector $T_i$.
- Generate an initial population of size $N_P$ for the parameter vector
$\Theta$ from the entire possible ranges.
- For each iteration, repeat the steps below.
    1. For each member in the current population, simulate a time series $j$ and
calculate its feature vector $F_j$.
    2. Calculate the fitness value for each member: $$\text{Fitness}(j) = - ||F_j-T_i||.$$
    3. Produce the new generation based on the crossover, mutation and the survival.
- Upon convergence, we keep the cloest time series.




## 

\centerline{\includegraphics[width=1.3\textwidth]{figures/TSgenerationApp}}



# Forecasting based on features


## Time series forecasting methods


\metroset{block=fill} 
\begin{alertblock}{The six forecasting methods.}
\begin{enumerate}
\item Naïve: using the most recent observation as the forecast.
\item Seasonal naïve: forecasts are equal to the most recent observation from the corresponding time of year. 
\item The Theta method, which performed particularly well in the M3-Competition. 
\item ETS: exponential smoothing state space modelling.
\item ARIMA: autoregressive integrated moving average models.
\item STL-AR: an AR model is fitted to the seasonally adjusted series, while the seasonal component is forecast using Seasonal naïve. 
\end{enumerate} 
\end{alertblock}

## Modelling features and forecasting performances on training data

$$\bf{MASE}_{N\times6} \Leftrightarrow \bf{F}_{N\times p}$$

$$\mathbf{MASE^{(i)}} = f_1^{(i)}(F_1) + f_2^{(i)}(F2) + ... + f_p^{(i)}(F_p) + \epsilon^{(i)}$$

## Apply the model on the forecasts on M3


\centerline{\includegraphics[width=\textwidth]{figures/M3MASE.pdf}}  


## Conclusions

- Feature description.
- Time series simulation from MAR models.
- 2D space (identify unusual time series, find clusters, etc.).
- Develop meta-forecasting algorithms which choose a specific method based on the location of a time series in the instance space.
- Generate new time series with specific features. 


## Next step 

- Find boundaries of the spaces.
- Explore other DGPs.
- Investigate other optimization algorithms for time series generation.
- Extension to multivariate time series.


## Thanks!


\Large http://feng.li

\Large feng.li@cufe.edu.cn


## References 
\fontsize{9}{10}\sf
